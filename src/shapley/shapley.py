from itertools import compress
from pathlib import Path

import numpy as np
import pandas as pd

# from utils import gen_type, shap_alloc_helper, char_order, to_bool_list
from utils.utils import char_order, gen_type, shap_alloc_helper, to_bool_list, \
    ceildiv, divide_chunks, flatten


#################################
    #PREPROCESSING: TAILORED FOR THE SPECIFIC INPUT
#################################

def gen_cohorts(num_scen: int, num_cohorts: int, folder_path: str, area_fname: str):
    """Given the number of scenarios, the number of cohorts to generate
    a folder path of the scenarios generated by vatic that include 
    the production amount of each generator, and an area mapping file name, 
    this function generates the cohorts for testing the shapley allocations
    
    Parameters
    -----------
    num_scen: int 
        the number of scenarios, this should match the number of files in
        folder path
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    folder_path: string
        the folder containing the scenario generations
    area_fname: string
        the name of the file containing the area mapping corresponding
        to the generator name

    Returns
    -------
    Dataframe
        A dataframe containing the reformatted file containing the required
        ingredients to compute the shapley allocation
    Also saves a text file of the cohort list
    """

    #Make the folder into a path object
    folder_path = Path(folder_path)

    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))

    #Create a df to hold the total emissions for each hour and scenario
    full_em_df = pd.DataFrame()
    
    #Loop through all the csv files
    for i in range(len(files_list)):
        #For each scenario file, open and read it into a temporary df. Then append it to the 
        #previous data frame
        temp_df = pd.read_table(files_list[i], sep = ",",
                         header = 0, index_col = False,
                          usecols = ["Date", "Hour", "Generator", "CO2 Emissions metric ton", 
                                     "Dispatch"])
        #label the scenario number to keep track
        temp_df["Scenario"] = files_list[i].name[5:9]
        full_em_df =  pd.concat([full_em_df, temp_df], ignore_index = True, sort=False)
    
    #Create new column with generator type
    full_em_df["Type"] = full_em_df["Generator"].apply(lambda x: gen_type(x))
    
    del temp_df

    #Open up the areas file 
    #import the data
    gen_zone_df = pd.read_table(area_fname, sep = ",",
                         header = 0, index_col = False)
    #Merge the two data sets
    full_em_df = full_em_df.merge(gen_zone_df, how = "left",
                                  left_on = "Generator", right_on = "GEN UID", copy = False)

    #Sort the dataframe by area
    #full_em_df['Area'] = full_em_df['Area'].astype('str') 
    full_em_df.sort_values(by = ["Area"], axis = 0,inplace= True)
    
    #use the areas file to determine the number of generators
    num_gen = len(gen_zone_df.index)

    #Generate the cohort list
    #Determine the size of each cohort based on number of generators and number
    #of cohorts. Have to round up
    cohort_size = ceildiv(num_gen, num_cohorts)

    #Create the cohorts
    cohort = list(divide_chunks(gen_zone_df["GEN UID"].values, cohort_size))
    del gen_zone_df

    #Create a new vector holding the characteristic function matrix
    char_df = pd.DataFrame({"Hour": full_em_df["Hour"], "Type": full_em_df["Type"], 
                            "Generator": full_em_df["Generator"],
                            "Area": full_em_df["Area"], "Scenario": full_em_df["Scenario"]})
    #Which column are we using to determine the cohorts
    category = "Generator"
    #Create the matrix of repeated C02 emissions metric ton
    emission_mat = np.tile(full_em_df["CO2 Emissions metric ton"].values,(2**num_cohorts,1))
    #Create the matrix of repeated dispatch
    dispatch_mat = np.tile(full_em_df["Dispatch"].values,(2**num_cohorts,1))
    #Merge these two together
    temp_mat = np.concatenate((emission_mat, dispatch_mat), axis = 0)
    
    #Check to make sure the merge happened correctly
    del emission_mat, dispatch_mat
    
    #Convert into  dataframe
    temp_df = pd.DataFrame(data = np.transpose(temp_mat),
                           columns = [i + "e" for i in char_order(num_cohorts)] + 
                           [i + "d" for i in char_order(num_cohorts)], 
                           index = range(len(full_em_df["CO2 Emissions metric ton"].values)))
    
    #Merge the two data frames togetjer
    char_df = pd.concat([char_df, temp_df], axis = 1)
    del temp_df
    
    #Plug in the 0s for idealized cases
    #skip the first one which is the BAU case
    for i in char_order(num_cohorts)[1:]:
        #Change the value of the new emission column and dispatch column to 0
        char_df.loc[char_df[category].isin(flatten(list(compress(cohort,to_bool_list(i))))),
                    [i + "e", i + "d"]] = 0
    #Convert the last column back into t

    #Aggregate the data to scenario level to get the contribution values by scenario
    #Group the data frame by scenario and hour. The aggregation only requires summing
    #the allocations and the total emissions
    scen_df = char_df.drop(["Type", "Area", "Generator"],
                           axis = 1).groupby(["Scenario", "Hour"]).sum()
    #Convert the last column back into nonzeros so we dont divide by 0
    scen_df[char_order(num_cohorts)[-1] + "d"] = scen_df[char_order(num_cohorts)[0] + "d"]
    
    #Compute the actual payoff value
    scen_df = scen_df.iloc[:,0:(2**num_cohorts)]/ scen_df.iloc[:,(2**num_cohorts):(2**(num_cohorts+1))].values
    #Rename columns
    scen_df.columns = char_order(num_cohorts)
    #Change all nonporucing assets from Nan to zeros
    scen_df = scen_df.fillna(0)

    #Save the cohorts as a text file
    with open("cohort.txt", "w") as output:
        output.write(str(cohort))
    return(scen_df)

#################################
#ANALAYSIS - STAYS THE SAME FOR ALL INPUTS
#################################
def shap_alloc(num_scen: int, alpha: float, num_cohorts: int, scen_df: object):
    """Given the number of scenarios, the alpha used to compute the CVar,
    the number of cohorts for asser allocation computation, the
    scenario dataframe generated in the previous code, then we compute the
    asset allocation
    
    Parameters
    -----------
    num_scen: int 
        the number of scenarios, this should match the number of files in
        folder path
    alpha: float
        a float between 0 and 1 that corresponds to how much of the tail
        of the total allocations that we sample for the CVaR computation
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    scen_df: obj
        the dataframe generated above that contains all information needed
        to find the asset allocation

    Returns
    -------
    Dataframe
        A dataframe containing the the cohort allocations
    """

    #Define the quantile as the inverted alpha
    quantile = 1 - alpha
    #First get all the column vectors needed for shapley allocation
    #We convert into an array because computation requires matrix multiplication
    temp_mat = np.array([shap_alloc_helper(i+1, num_cohorts) for i in range(num_cohorts)])
    #Compute the shapley allocations
    scen_df = pd.concat([pd.DataFrame(data = np.matmul(scen_df.values, 
                                            np.transpose(temp_mat)), 
                           columns = ["aloc" + str(i +1) for i in range(num_cohorts)], 
                           index = scen_df.index),
                         scen_df], axis =1 )

    #Compute the 95th percentile for each hour accross all scenarious in the BAU case
    scen_df["VaR"] = scen_df[char_order(num_cohorts)[0]].groupby(level = "Hour").transform("quantile",
                                                                                 quantile)

    #Create a column that indicates if in the tail
    scen_df["inTail"] = scen_df[char_order(num_cohorts)[0]] >= scen_df["VaR"]

    result_df = round(scen_df.loc[scen_df["inTail"], ["aloc" + str(i+1) for i in range(num_cohorts)] + [char_order(num_cohorts)[0]]]\
                .groupby(level = "Hour").sum() / (num_scen * alpha),2)
    return(result_df)

