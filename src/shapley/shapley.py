from itertools import compress
from pathlib import Path

import numpy as np
import pandas as pd

# from utils import gen_type, shap_alloc_helper, char_order, to_bool_list
from utils.utils import char_order, gen_type, shap_alloc_helper, to_bool_list, \
    ceildiv, divide_chunks, flatten


#################################
    #PREPROCESSING: TAILORED FOR THE SPECIFIC INPUT
#################################

def gen_cohorts(num_cohorts: int, folder_path: str, area_fname: str):
    """Given the number of scenarios, the number of cohorts to generate
    a folder path of the scenarios generated by vatic that include 
    the production amount of each generator, and an area mapping file name, 
    this function generates the cohorts for testing the shapley allocations
    
    Parameters
    -----------
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    folder_path: string
        the folder containing the scenario generations
    area_fname: string
        the name of the file containing the area mapping corresponding
        to the generator name

    Returns
    -------
    Dataframe
        A dataframe containing the reformatted file containing the required
        ingredients to compute the shapley allocation
    Also saves a text file of the cohort list
    """

    #Make the folder into a path object
    folder_path = Path(folder_path)

    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))
    
    #Open up the areas file 
    #import the data
    gen_zone_df = pd.read_table(area_fname, sep = ",",
                         header = 0, index_col = False)
    #use the areas file to determine the number of generators
    num_gen = len(gen_zone_df.index)

    #Generate the cohort list
    #Determine the size of each cohort based on number of generators and number
    #of cohorts. Have to round up
    cohort_size = ceildiv(num_gen, num_cohorts)

    #Which column are we using to determine the cohorts
    category = "Generator"
    #Create the cohorts
    cohort = list(divide_chunks(gen_zone_df["GEN UID"].values, cohort_size))
    del gen_zone_df

    #Create a list containing the 2^n binary representation of cohorts
    #where a 1 indicates that cohort 1 is included
    char_labels = char_order(num_cohorts)

    #Create a list to hold all the allocations for the 24 hours
    temp = []
    #Loop through all the csv files
    for f in range(len(files_list)):
        #For each scenario file, open and read it into a df
        em_df = pd.read_table(files_list[f], sep = ",",
                         header = 0, index_col = False,
                          usecols = ["Date", "Hour", "Generator", "CO2 Emissions metric ton", 
                                     "Dispatch"])
        #label the scenario number to keep track
        scen_num = files_list[f].name[5:9]
        #full_em_df =  pd.concat([full_em_df, temp_df], ignore_index = True, sort=False)
        #Create new column with generator type
        em_df["Type"] = em_df["Generator"].apply(lambda x: gen_type(x))
        
        #Then loop across hours
        for h in range(24):
            #Create a vector that holds the total emissions for each cohort
            cohort_em = np.zeros(num_cohorts)
            #Create a vector that holds the total output for each cohort
            cohort_dispatch = np.zeros(num_cohorts)
            for i in range(num_cohorts):
                cohort_em[i], cohort_dispatch[i] = em_df.loc[(em_df[category].isin(cohort[i])) & (em_df["Hour"] == h),
                                                            ["CO2 Emissions metric ton", "Dispatch"]].sum(axis = 0).values
            
            #Create a vector of size 2^n that holds the allocation of efficient
            char_values = np.zeros(2**num_cohorts)
            
            for i in range((2**num_cohorts) - 1):
                char_values[i] = np.divide(np.sum(cohort_em[to_bool_list(char_labels[i])]),
                                                np.sum(cohort_dispatch[to_bool_list(char_labels[i])]))
            temp.append([scen_num, h] + char_values.tolist())

    #Create a df to store all the scenarios allocation values
    full_em_df = pd.DataFrame(data = temp, 
                                columns = ["Scenario", "Hour"]  + char_labels)

    full_em_df.set_index(["Scenario", "Hour"], inplace = True)
    #Change all nonporucing assets from Nan to zeros
    full_em_df = full_em_df.fillna(0)
 
    #Save the cohorts as a text file
    with open("cohort.txt", "w") as output:
        output.write(str(cohort))
    return(full_em_df)

#################################
#ANALAYSIS - STAYS THE SAME FOR ALL INPUTS
#################################
def shap_alloc(num_scen: int, alpha: float, num_cohorts: int, scen_df: object):
    """Given the number of scenarios, the alpha used to compute the CVar,
    the number of cohorts for asser allocation computation, the
    scenario dataframe generated in the previous code, then we compute the
    asset allocation
    
    Parameters
    -----------
    num_scen: int 
        the number of scenarios, this should match the number of files in
        folder path
    alpha: float
        a float between 0 and 1 that corresponds to how much of the tail
        of the total allocations that we sample for the CVaR computation
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    scen_df: obj
        the dataframe generated above that contains all information needed
        to find the asset allocation

    Returns
    -------
    Dataframe
        A dataframe containing the the cohort allocations
    """

    #Define the quantile as the inverted alpha
    quantile = 1 - alpha
    #First get all the column vectors needed for shapley allocation
    #We convert into an array because computation requires matrix multiplication
    temp_mat = np.array([shap_alloc_helper(i+1, num_cohorts) for i in range(num_cohorts)])
    #Compute the shapley allocations
    scen_df = pd.concat([pd.DataFrame(data = np.matmul(scen_df.values, 
                                            np.transpose(temp_mat)), 
                           columns = ["aloc" + str(i +1) for i in range(num_cohorts)], 
                           index = scen_df.index),
                         scen_df[char_order(num_cohorts)[0]]], axis =1 )

    #Compute the 95th percentile for each hour accross all scenarious in the BAU case
    scen_df["VaR"] = scen_df[char_order(num_cohorts)[0]].groupby(level = "Hour").transform("quantile",
                                                                                 quantile, interpolation = "lower")

    #Create a column that indicates if in the tail
    scen_df["inTail"] = scen_df[char_order(num_cohorts)[0]] > scen_df["VaR"]

    result_df = round(scen_df.loc[scen_df["inTail"], ["aloc" + str(i+1) for i in range(num_cohorts)] + [char_order(num_cohorts)[0]]]\
                .groupby(level = "Hour").sum() / (num_scen * alpha),2)
    return(result_df)

