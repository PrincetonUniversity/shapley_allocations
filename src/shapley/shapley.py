from pathlib import Path

import numpy as np
import pandas as pd
from utils.utils import (
    char_order,
    divide_chunks2,
    shap_Phi,
    to_bool_list,
    output,
    sum_tail,
    split)

#################################
#PREPROCESSING: TAILORED FOR THE SPECIFIC INPUT
#################################
def gen_df(folder_path: str, num_files: int, 
           indx: list, output_cols: list):
    """Generates the dataframe with appropriate output values for each scenario
    to call the CVAR 
    
    Parameters
    -----------
    folder_path: str
        the name of the folder path containing the scenario files
    num_files: int
        the number of files in the folder path
    indx: list
        contains all the indices of the file
    output_cols: list
        a list containing the column names to perform computation on


    Returns
    -------
    Dataframe
        A dataframe with the newly generated output columns
    """
    #Make the folder into a path object
    folder_path = Path(folder_path)

    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))

    #Create a list to hold all the allocations for the 24 hours
    output_df = []
    #Loop through all the csv files
    for f in range(num_files):
        #label the scenario number to keep track
        scen_num = files_list[f].name[5:9]
        #For each scenario file, open and read it into a df
        em_df = pd.read_table(files_list[f], sep = ",", header = 0, index_col = indx)
        #Instead of looping at each hour, simply do a group by
        #Then call the appropriate function to complete the computation
        computed_df = output(em_df[output_cols].groupby(level = 0), output_cols)
        
        #Create multi - index
        #Input all into the list
        output_df= output_df + (list(zip([scen_num]* len(computed_df.index.values), 
                                  computed_df.index.to_list(), 
                                  computed_df.values.tolist())))
    #Create a df to store all the scenarios allocation values
    full_em_df = pd.DataFrame(data = output_df,  columns = ["Scenario"] +indx + ["Output"])

    full_em_df.set_index(indx, inplace = True)
    #Change all nonproducing assets from Nan to zeros
    full_em_df.fillna(0, inplace = True)
    return(full_em_df)

def gen_cohort(num_cohorts: int, area_fname: str, indx: str):
    """Given the number of clusters desired, and a file name specifying
    each generator, generates the cohorts for testing the speed of shapley allocations
    
    Parameters
    -----------
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    area_fname: str
        the name of the file containing the area mapping corresponding
        to the generator name
    indx: list
        string list of the column name that contains the id of the generators

    Returns
    -------
    list
        A deterministic list of clusters identified by the generator name
    """
    #Open up the areas file 
    #import the data
    gen_zone_df = pd.read_table(area_fname, sep = ",", header = 0, 
                                index_col = indx)
    #Determine the number of generators
    #num_gen = len(gen_zone_df.index)

    #Generate the cohort list
    #cohort = list(divide_chunks2(gen_zone_df.index.to_list(), num_cohorts))
    cohort = list(split(gen_zone_df.index.to_list(), num_cohorts))
    #Save the cohorts as a text file
    with open("cohort.txt", "w") as output:
        output.write(str(cohort))
    return(cohort)

def gen_cohort_payoff(num_cohorts: int, cohorts: list, folder_path: str, 
               tail_scen: list, gen_id_col: str, indx: list, output_cols: list):
    """Given the number of clusters, the respective list of list containing the
    ID of the clusters, a folder path of the scenarios generated by vatic, 
    the column containing the IDs of the clusters, the scenarios of interest,
    the identifying column of the data, the indices to be used when importing 
    data and the columns used for computation, create the characteristic 
    vector for each tail scenario
    
    Parameters
    -----------
    num_cohorts: int
        the number of clusters that we eventually perform shapley allocations on
    cohorts: list
        list of list, with each list containing ID of the generators in the clusters
    folder_path: str
        the name of the folder path containing the scenario files
    tail_scen: list
        list of indices of tail scenarios
    gen_id_col: str
        in each of the scenario files, what is the column name for the 
        column that contains the ID
    indx: list
        contains all the indices of the file
    output_cols: list
        a list containing the column names to perform computation on


    Returns
    -------
    np.ndarray
        A multi-dim array containing the characteristic functions
    """
    #Open all emission files
    #Make the folder into a path object
    folder_path = Path(folder_path)

    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))

    em_df = (pd.read_csv(f, sep = ",", usecols = [gen_id_col, indx[1]] + output_cols,
                         header = 0).assign(Scenario = f.name[5:9]) for f in files_list)
    
    #Combine all the csvs into one
    scen_em_df = pd.concat(em_df, ignore_index=True)
    scen_em_df.set_index(indx, inplace= True)
    del em_df

    #Filter for only the tail scenarios
    scen_em_df = scen_em_df.loc[tail_scen,:]

    #Create a column that identifies which cohort the generator is in
    #Labels start with 0
    for i in range(num_cohorts):
        scen_em_df.loc[scen_em_df[gen_id_col].isin(cohorts[i]), "in_cohort"] = int(i)

    #Reset the index to do the groupby
    #scen_em_df.reset_index(inplace = True)

    #Create a list containing the 2^n binary representation of cohorts
    #where a 1 indicates that cohort 1 is included
    char_labels = char_order(num_cohorts)

    #Group by cohort then try to relate it to the characteristic labels
    #computed_df = scen_em_df[["in_cohort"] + indx + output_cols].groupby(by = indx + ["in_cohort"]).sum()
    computed_df = scen_em_df.groupby(by = indx + ["in_cohort"]).sum()

    #Create a 3dmatrix of size 2^n that holds the allocations of each coalition
    tail_indx =computed_df.index.unique(level = indx[0])
    char_values = np.zeros((len(tail_indx),
                            2**num_cohorts,
                            len(computed_df.index.unique(level = indx[1]))))
    #Fill in the vector
    #Want to only filter from the second level. Then reset the index at second level
    for s in range(len(tail_indx)):
        scen_df = computed_df.loc[(computed_df.index.get_level_values(indx[0]) == tail_indx[s],
                                    slice(None), slice(None))]
        for n in range((2**num_cohorts) - 1):
            coal_df = scen_df.loc[(slice(None),
                                    slice(None),
                                    computed_df.index.unique(level = 2)[to_bool_list(char_labels[n])]),
                                    :].reset_index()
            output_results = output(coal_df[[indx[1]] + output_cols].groupby(indx[1]), output_cols)
            char_values[s,n,output_results.index.values] =  output_results.values
    #return the dataframe
    return(np.transpose(char_values,(0, 2, 1)))
    #return(pd.DataFrame(data = np.transpose(char_values), columns =char_labels, 
     #                   index=coal_df[indx[1]]))    

#################################
#ANALAYSIS - STAYS THE SAME FOR ALL INPUTS
#################################
def tail_sims(alpha: float, scen_df: object, indx: list, output_col: str):
    """Given the alpha used to compute the CVar, the scenario dataframe 
    with the outputs to compute the cvar from, dim of data
    and the output column name outputs the tail scenarios 
    
    Parameters
    -----------
    alpha: float
        a float between 0 and 1 that corresponds to how much of the tail
        of the total allocations that we sample for the CVaR computation
    scen_df: obj
        the dataframe generated above that has the output values
    grp_by_col: str
        the column name to group by and use to find the quantiles
    scen_col: str
        the column name to identify the tail risks
    output_col: str
        string name of the output column

    Returns
    -------
    list
        scenarios that are in the tail
    """
    #Define the quantile as the inverted alpha
    quantile = 1 - alpha
    #ALways want to group by the lowest index level
    var_df = scen_df[output_col].groupby(by = indx[1]).transform("quantile",
                                                                    quantile, interpolation = "lower")
    #return hour and their corresponding scenarios
    return(list(zip(scen_df.loc[scen_df[output_col] > var_df, indx[0]].values.tolist(), 
                    scen_df.loc[scen_df[output_col] > var_df, indx[0]].index.to_list())))

def shap(char_mat: np.ndarray, num_cohorts: int, num_tail_scen: int):
    """Given the characteristic matrix and the number of cohorts,
    compute the shapley
    
    Parameters
    -----------
    char_mat: np.ndarray 
        a 3d matrix containing all the characteric values. The Cv in the paper
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name

    Returns
    -------
    Dataframe
        A dataframe containing the the cohort allocations
    """
    return(sum_tail(np.matmul(char_mat, shap_Phi(num_cohorts)), 
                    num_tail_scen))
                                                               
'''
temp = gen_df("/Users/felix/Programs/orfeus/2018-08-01/emission", 10, 
       ["Hour"], ["CO2 Emissions metric ton", "Dispatch"])

tail = tail_sims(0.05, temp, "Hour", "Scenario", "Output")

cohort = gen_cohort(2, "/Users/felix/Github/shapley/texas7k_2020_gen_to_zone.csv",
                    "GEN UID")
test = (gen_cohort_payoff(2, cohort, "/Users/felix/Programs/orfeus/2018-08-01/emission",
                  tail, "Generator", ["Scenario", "Hour"],
                  ["CO2 Emissions metric ton", "Dispatch"]))
print(shap(test,2))
print(sum_tail(shap(test,2),1))
'''