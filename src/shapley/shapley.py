from pathlib import Path

import numpy as np
import pandas as pd
#import glob
from utils.utils import (
    ceildiv,
    char_order,
    divide_chunks,
    shap_Phi,
    gen_type,
    shap_alloc_helper,
    to_bool_list,
    output
)

#################################
#PREPROCESSING: TAILORED FOR THE SPECIFIC INPUT
#################################
def gen_df(folder_path: str, num_files: int, 
           indx: list, output_cols: list):
    """Generates the dataframe with appropriate output values for each scenario
    to call the CVAR 
    
    Parameters
    -----------
    folder_path: str
        the name of the folder path containing the scenario files
    num_files: int
        the number of files in the folder path
    indx: list
        contains all the indices of the file
    output_cols: list
        a list containing the column names to perform computation on


    Returns
    -------
    Dataframe
        A dataframe with the newly generated output columns
    """
    #Make the folder into a path object
    folder_path = Path(folder_path)

    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))

    #Create a list to hold all the allocations for the 24 hours
    output_df = []
    #Loop through all the csv files
    for f in range(num_files):
        #label the scenario number to keep track
        scen_num = files_list[f].name[5:9]
        #For each scenario file, open and read it into a df
        em_df = pd.read_table(files_list[f], sep = ",", header = 0, index_col = indx)
        #Instead of looping at each hour, simply do a group by
        #Then call the appropriate function to complete the computation
        computed_df = output(em_df[output_cols].groupby(level = 0), output_cols)
        
        #Create multi - index
        #Input all into the list
        output_df= output_df + (list(zip([scen_num]* len(computed_df.index.values), 
                                  computed_df.index.to_list(), 
                                  computed_df.values.tolist())))
    #Create a df to store all the scenarios allocation values
    full_em_df = pd.DataFrame(data = output_df,  columns = ["Scenario"] +indx + ["Output"])

    full_em_df.set_index(indx, inplace = True)
    #Change all nonproducing assets from Nan to zeros
    full_em_df.fillna(0, inplace = True)
    return(full_em_df)

def gen_cohort(num_cohorts: int, area_fname: str, indx: list):
    """Given the number of clusters desired, and a file name specifying
    each generator, generates the cohorts for testing the speed of shapley allocations
    
    Parameters
    -----------
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    area_fname: str
        the name of the file containing the area mapping corresponding
        to the generator name
    indx: list
        string list of the column name that contains the id of the generators

    Returns
    -------
    list
        A deterministic list of clusters identified by the generator name
    """
    #Open up the areas file 
    #import the data
    gen_zone_df = pd.read_table(area_fname, sep = ",", header = 0, 
                                index_col = indx)
    #Determine the number of generators
    num_gen = len(gen_zone_df.index)

    #Generate the cohort list
    #Determine the size of each cohort based on number of generators and number
    #of cohorts. Have to round up
    cohort_size = ceildiv(num_gen, num_cohorts)

    #Create the cohorts
    #cohort = [gen_zone_df.index[i] for i in list(divide_chunks(num_gen, cohort_size))]
    cohort = list(divide_chunks(gen_zone_df.index.to_list(), cohort_size))
    return(cohort)

def gen_cohort_payoff(num_cohorts: int, cohorts: list, folder_path: str, 
               tail_scen: list, gen_id_col: str, indx: list, output_cols: list):
    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    #files_list = glob.iglob(os.path.join(folder_path, "*.csv"))
    
    #scen_em_df = (pd.read_csv(file).assign({"Scenario": = file.split('.')[0]}) for file in files_list)
    #Make the folder into a path object
    folder_path = Path(folder_path)

    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))

    em_df = (pd.read_csv(f, sep = ",", usecols = [gen_id_col, indx[1]] + output_cols,
                         header = 0).assign(Scenario = f.name[5:9]) for f in files_list)
    
    #Combine
    scen_em_df = pd.concat(em_df, ignore_index=True)
    scen_em_df.set_index(indx, inplace= True)
    del em_df

    #Filter for only the tail scenarios
    scen_em_df = scen_em_df.loc[tail_scen,:]
    #Reset the index to do the groupby
    scen_em_df.reset_index(inplace = True)

    #Create a list containing the 2^n binary representation of cohorts
    #where a 1 indicates that cohort 1 is included
    char_labels = char_order(num_cohorts)

    #Create a column that identifies which cohort the generator is in
    #Labels start with 0
    for i in range(num_cohorts):
        scen_em_df.loc[scen_em_df[gen_id_col].isin(cohorts[i]), "in_cohort"] = int(i)
    
    #Group by cohort then try to relate it to the characteristic labels
    computed_df = scen_em_df[["in_cohort", indx[1]] + output_cols].groupby(by = [indx[1], "in_cohort"]).sum()

    #Create a vector of size 2^n that holds the allocations of each coalition
    char_values = np.zeros((2**num_cohorts,
                            len(computed_df.index.unique(level = indx[1]))))
    #Fill in the vector
    #Want to only filter from the second level. Then reset the index at second level
    for n in range((2**num_cohorts) - 1):
        coal_df = computed_df.loc[(slice(None), 
                                   computed_df.index.unique(level = 1)[to_bool_list(char_labels[n])]),
                                   :].reset_index()
        char_values[n,:] =  output(coal_df[[indx[1]] + output_cols].groupby(indx[1]), output_cols)
    #return the dataframe
    return(np.transpose(char_values))
    #return(pd.DataFrame(data = np.transpose(char_values), columns =char_labels, 
     #                   index=coal_df[indx[1]]))    


def process_df(num_cohorts: int, cohorts: list, folder_path: str, 
               num_files: int, gen_id_col: str, indx: list, output_cols: list):
    """Given the number of clusters, the respective list of list containing the
    ID of the clusters, a folder path of the scenarios generated by vatic, 
    the column containing the IDs of the clusters, the number of files in the folder
    the index this function generates the cohorts for testing the shapley allocations
    
    Parameters
    -----------
    num_cohorts: int
        the number of clusters that we eventually perform shapley allocations on
    cohorts: list
        list of list, with each list containing ID of the generators in the clusters
    folder_path: str
        the name of the folder path containing the scenario files
    num_files: int
        the number of files in the folder path
    gen_id_col: str
        in each of the scenario files, what is the column name for the 
        column that contains the ID
    indx: list
        contains all the indices of the file
    output_cols: list
        a list containing the column names to perform computation on


    Returns
    -------
    Dataframe
        A dataframe with the newly generated output columns
    """
    #Make the folder into a path object
    folder_path = Path(folder_path)

    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))

    #Create a list containing the 2^n binary representation of cohorts
    #where a 1 indicates that cohort 1 is included
    char_labels = char_order(num_cohorts)

    #Create a list to hold all the allocations for the 24 hours
    scen_em_df = []
    #Loop through all the csv files
    for f in range(num_files):
        #For each scenario file, open and read it into a df
        em_df = pd.read_table(files_list[f], sep = ",", header = 0, index_col = indx)
        
        #label the scenario number to keep track
        scen_num = files_list[f].name[5:9]
        
        #Then loop across the finest index level
        for h in em_df.index.values:
            #Create a vector that holds the total emissions for each cohort
            cohort_em = np.zeros(num_cohorts)
            #Create a vector that holds the total output for each cohort
            cohort_dispatch = np.zeros(num_cohorts)
            for i in range(num_cohorts):
                cohort_em[i], cohort_dispatch[i] = em_df.loc[(em_df[id_col].isin(cohorts[i])) & (em_df.index == h),
                                                            comp_cols].sum(axis = 0).values
            
            #Create a vector of size 2^n that holds the allocations of each cluster
            char_values = np.zeros(2**num_cohorts)
            
            for i in range((2**num_cohorts) - 1):
                char_values[i] = np.divide(np.sum(cohort_em[to_bool_list(char_labels[i])]),
                                                np.sum(cohort_dispatch[to_bool_list(char_labels[i])]))
            scen_em_df.append([scen_num, h] + char_values.tolist())

    #Create a df to store all the scenarios allocation values
    full_em_df = pd.DataFrame(data = scen_em_df,  columns = indx  + char_labels)

    full_em_df.set_index(indx, inplace = True)
    #Change all nonporucing assets from Nan to zeros
    full_em_df = full_em_df.fillna(0)
 


    

def gen_cohorts(num_cohorts: int, folder_path: str, area_fname: str):
    """Given the number of scenarios, the number of cohorts to generate
    a folder path of the scenarios generated by vatic that include 
    the production amount of each generator, and an area mapping file name, 
    this function generates the cohorts for testing the shapley allocations
    
    Parameters
    -----------
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    folder_path: string
        the folder containing the scenario generations
    area_fname: string
        the name of the file containing the area mapping corresponding
        to the generator name

    Returns
    -------
    Dataframe
        A dataframe containing the reformatted file containing the required
        ingredients to compute the shapley allocation
    Also saves a text file of the cohort list
    """

    #Make the folder into a path object
    folder_path = Path(folder_path)

    #Open all emission files
    #iterate over directory and store filenames in a list
    #Only keep the csv files and put file names in a list
    files_list = list(filter(lambda f: f.name.endswith('.csv'), folder_path.iterdir()))
    
    #Open up the areas file 
    #import the data
    gen_zone_df = pd.read_table(area_fname, sep = ",", header = 0, index_col = False)
    #use the areas file to determine the number of generators
    num_gen = len(gen_zone_df.index)

    #Generate the cohort list
    #Determine the size of each cohort based on number of generators and number
    #of cohorts. Have to round up
    cohort_size = ceildiv(num_gen, num_cohorts)

    #Which column are we using to determine the cohorts
    category = "Generator"
    
    #Create the cohorts
    cohort = list(divide_chunks(gen_zone_df["GEN UID"].values, cohort_size))
    del gen_zone_df

    #Create a list containing the 2^n binary representation of cohorts
    #where a 1 indicates that cohort 1 is included
    char_labels = char_order(num_cohorts)

    #Create a list to hold all the allocations for the 24 hours
    scen_em_df = []
    #Loop through all the csv files
    for f in range(len(files_list)):
        #For each scenario file, open and read it into a df
        em_df = pd.read_table(files_list[f], sep = ",", header = 0, index_col = False,
                              usecols = ["Date", "Hour", "Generator", "CO2 Emissions metric ton", "Dispatch"])
        
        #label the scenario number to keep track
        scen_num = files_list[f].name[5:9]
        
        #Create new column with generator type
        em_df["Type"] = em_df["Generator"].apply(lambda x: gen_type(x))
        
        #Then loop across hours
        for h in range(24):
            #Create a vector that holds the total emissions for each cohort
            cohort_em = np.zeros(num_cohorts)
            #Create a vector that holds the total output for each cohort
            cohort_dispatch = np.zeros(num_cohorts)
            for i in range(num_cohorts):
                cohort_em[i], cohort_dispatch[i] = em_df.loc[(em_df[category].isin(cohort[i])) & (em_df["Hour"] == h),
                                                            ["CO2 Emissions metric ton", "Dispatch"]].sum(axis = 0).values
            
            #Create a vector of size 2^n that holds the allocation of efficient
            char_values = np.zeros(2**num_cohorts)
            
            for i in range((2**num_cohorts) - 1):
                char_values[i] = np.divide(np.sum(cohort_em[to_bool_list(char_labels[i])]),
                                                np.sum(cohort_dispatch[to_bool_list(char_labels[i])]))
            scen_em_df.append([scen_num, h] + char_values.tolist())

    #Create a df to store all the scenarios allocation values
    full_em_df = pd.DataFrame(data = scen_em_df,  columns = ["Scenario", "Hour"]  + char_labels)

    full_em_df.set_index(["Scenario", "Hour"], inplace = True)
    #Change all nonporucing assets from Nan to zeros
    full_em_df = full_em_df.fillna(0)
 
    #Save the cohorts as a text file
    with open("cohort.txt", "w") as output:
        output.write(str(cohort))
    return(full_em_df)

#################################
#ANALAYSIS - STAYS THE SAME FOR ALL INPUTS
#################################
def tail_sims(alpha: float, scen_df: object, grp_by_col : str, 
              scen_col: str, output_col: str):
    """Given the alpha used to compute the CVar, the scenario dataframe 
    with the outputs to compute the cvar from, dim of data
    and the output column name outputs the tail scenarios 
    
    Parameters
    -----------
    alpha: float
        a float between 0 and 1 that corresponds to how much of the tail
        of the total allocations that we sample for the CVaR computation
    scen_df: obj
        the dataframe generated above that has the output values
    grp_by_col: str
        the column name to group by and use to find the quantiles
    scen_col: str
        the column name to identify the tail risks
    output_col: str
        string name of the output column

    Returns
    -------
    list
        scenarios that are in the tail
    """
    #Define the quantile as the inverted alpha
    quantile = 1 - alpha
    #ALways want to group by the lowest index level
    var_df = scen_df[output_col].groupby(by = grp_by_col).transform("quantile",
                                                                    quantile, interpolation = "lower")
    #return hour and their corresponding scenarios
    return(list(zip(scen_df.loc[scen_df[output_col] > var_df, scen_col].values.tolist(), 
                    scen_df.loc[scen_df[output_col] > var_df, scen_col].index.to_list())))

def shap(char_df, num_cohorts: int):
    return(np.matmul(np.array(char_df), shap_Phi(num_cohorts)))
                                                               
def shap_alloc(num_scen: int, alpha: float, num_cohorts: int, scen_df: object):
    """Given the number of scenarios, the alpha used to compute the CVar,
    the number of cohorts for asser allocation computation, the
    scenario dataframe generated in the previous code, then we compute the
    asset allocation
    
    Parameters
    -----------
    num_scen: int 
        the number of scenarios, this should match the number of files in
        folder path
    alpha: float
        a float between 0 and 1 that corresponds to how much of the tail
        of the total allocations that we sample for the CVaR computation
    num_cohorts: int
        the number of cohorts to be generated from the files given. The cohorts
        will be generated by generator name
    scen_df: obj
        the dataframe generated above that contains all information needed
        to find the asset allocation

    Returns
    -------
    Dataframe
        A dataframe containing the the cohort allocations
    """

    #Define the quantile as the inverted alpha
    quantile = 1 - alpha
    #First get all the column vectors needed for shapley allocation
    #We convert into an array because computation requires matrix multiplication
    temp_mat = np.array([shap_alloc_helper(i+1, num_cohorts) for i in range(num_cohorts)])
    #Compute the shapley allocations
    scen_df = pd.concat([pd.DataFrame(data = np.matmul(scen_df.values, 
                                            np.transpose(temp_mat)), 
                           columns = ["aloc" + str(i +1) for i in range(num_cohorts)], 
                           index = scen_df.index),
                         scen_df[char_order(num_cohorts)[0]]], axis =1 )

    #Compute the percentile for each hour accross all scenarious in the BAU case
    scen_df["VaR"] = scen_df[char_order(num_cohorts)[0]].groupby(level = "Hour").transform("quantile",
                                                                                 quantile, interpolation = "lower")

    #Create a column that indicates if in the tail
    scen_df["inTail"] = scen_df[char_order(num_cohorts)[0]] > scen_df["VaR"]

    result_df = round(scen_df.loc[scen_df["inTail"], ["aloc" + str(i+1) for i in range(num_cohorts)] + [char_order(num_cohorts)[0]]]\
                .groupby(level = "Hour").sum() / (num_scen * alpha),2)
    return(result_df)

temp = gen_df("/Users/felix/Programs/orfeus/2018-08-01/emission", 10, 
       ["Hour"], ["CO2 Emissions metric ton", "Dispatch"])

tail = tail_sims(0.05, temp, "Hour", "Scenario", "Output")

cohort = gen_cohort(2, "/Users/felix/Github/shapley/texas7k_2020_gen_to_zone.csv",
                    "GEN UID")
test = (gen_cohort_payoff(2, cohort, "/Users/felix/Programs/orfeus/2018-08-01/emission",
                  tail, "Generator", ["Scenario", "Hour"],
                  ["CO2 Emissions metric ton", "Dispatch"]))

print(shap(test,2), test)